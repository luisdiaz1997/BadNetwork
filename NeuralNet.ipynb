{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "from minpy.context import cpu, gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Net:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = list()\n",
    "        \n",
    "    def add_layer(self, neurons, activation = 'sigmoid'):\n",
    "        self.layers.append( Layer(neurons, activation = activation))\n",
    "        \n",
    "    def set_input(self, inp):\n",
    "        self.inp = inp\n",
    "        self.train_inp = inp\n",
    "        \n",
    "        \n",
    "    def set_output(self, out):\n",
    "        self.out = out\n",
    "        self.train_out = out\n",
    "        \n",
    "    def feed_forward(self):\n",
    "        self.layers[0].set_input(self.train_inp)\n",
    "        for i in range(1, len(self.layers) ):\n",
    "            (self.layers[i]).set_input(self.layers[i-1].output)\n",
    "        \n",
    "        self.diff = self.layers[-1].output - self.train_out\n",
    "        \n",
    "    def back_propagation(self, lr):\n",
    "        \n",
    "        tensor = list()\n",
    "        tensor.append(self.diff)\n",
    "        for i in reversed(range(0, len(self.layers) - 1)):\n",
    "            \n",
    "            tensor.append(np.multiply(\n",
    "                np.matmul(tensor[-1], np.transpose(self.layers[i+1].weights)),\n",
    "                self.layers[i].d_output\n",
    "                ))\n",
    "        \n",
    "        tensor = tensor[::-1]\n",
    "        for i in range(len(tensor)):\n",
    "            self.layers[i].weights -= lr*np.matmul(np.transpose(self.layers[i].inp), tensor[i])\n",
    "            self.layers[i].bias -= lr*np.matmul(np.transpose(self.layers[i].A), tensor[i])\n",
    "        \n",
    "    def train(self, epochs, lr = 1, batch_size = None):\n",
    "        if batch_size is None:\n",
    "            batch_size = self.inp.shape[0]\n",
    "        partitions = int(self.inp.shape[0]/batch_size)\n",
    "        limits = np.linspace(0, self.inp.shape[0], partitions + 1, dtype= int)\n",
    "        \n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            print('epoch = ', epoch)\n",
    "            for partition in range(partitions):\n",
    "                \n",
    "                \n",
    "                self.train_inp = self.inp[int(limits[partition]):int(limits[partition + 1])]\n",
    "                self.train_out = self.out[int(limits[partition]):int(limits[partition + 1])]\n",
    "                \n",
    "                self.feed_forward()\n",
    "                self.back_propagation(lr)\n",
    "            \n",
    "            self.train_inp = self.inp\n",
    "            self.train_out = self.out\n",
    "            self.feed_forward()\n",
    "            self.cost = np.sum((self.diff**2))\n",
    "            print(self.cost)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        \n",
    "        return np.where(x < 0, 0, x )\n",
    "    \n",
    "    @staticmethod\n",
    "    def d_sigmoid(x):\n",
    "        a = Layer.sigmoid(x)\n",
    "        return a*(1-a)\n",
    "    \n",
    "    @staticmethod\n",
    "    def d_relu(x):\n",
    "        \n",
    "        return np.where(x<0, 0, 1)\n",
    "    \n",
    "    function = { 'sigmoid': (lambda x: Layer.sigmoid(x)), 'relu': (lambda x: Layer.relu(x))}\n",
    "    \n",
    "    d_function = { 'sigmoid': (lambda x: Layer.d_sigmoid(x)), 'relu': (lambda x: Layer.d_relu(x)) }\n",
    "\n",
    "\n",
    "    def __init__(self, neurons, inp = None, activation = 'sigmoid'):\n",
    "        self.init = False\n",
    "        self.neurons = neurons\n",
    "        self.inp = self.set_input(inp)\n",
    "        self.activation = activation\n",
    "        \n",
    "        \n",
    "    def set_input(self, inp):\n",
    "        if (inp is None):\n",
    "            return None\n",
    "        \n",
    "        if (self.init == False):\n",
    "            self.weights = np.random.rand(inp.shape[1], self.neurons)\n",
    "            print (self.weights)\n",
    "            self.bias = np.random.rand(1, self.neurons)\n",
    "            \n",
    "            self.init = True\n",
    "            \n",
    "        self.inp = inp\n",
    "        self.A = np.ones((inp.shape[0], 1))\n",
    "        self.linear = np.add(\n",
    "                        np.matmul(self.inp, self.weights), \n",
    "                        np.matmul(self.A, self.bias)\n",
    "                        )\n",
    "        self.output = Layer.function[self.activation](self.linear)\n",
    "        self.d_output = Layer.d_function[self.activation](self.linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Neural_Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.add_layer(100, activation = 'sigmoid')\n",
    "a.add_layer(1, activation = 'sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = np.array([[0, 0], [0, 1], [1,0], [1,1]])\n",
    "inp[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.array([[0], [1], [1], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.set_input(inp)\n",
    "a.set_output(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0\n",
      "[[0.55559431 0.2557361  0.45052085 0.11534155 0.896896   0.27847232\n",
      "  0.66468908 0.96120398 0.92320827 0.30977793 0.10768642 0.39618304\n",
      "  0.16969142 0.71843216 0.17225959 0.53258711 0.96771477 0.83502614\n",
      "  0.43094024 0.36313038 0.60004498 0.93370521 0.13860886 0.72084957\n",
      "  0.78646978 0.43884704 0.5271726  0.16841588 0.17156733 0.79034448\n",
      "  0.1875565  0.93920903 0.75929538 0.36226916 0.44369825 0.09130094\n",
      "  0.27223379 0.97144399 0.95507091 0.96257899 0.02742183 0.35839412\n",
      "  0.12554456 0.18463367 0.90954181 0.92588286 0.12137741 0.72894699\n",
      "  0.19054148 0.54792485 0.81381627 0.94561406 0.57753531 0.92721634\n",
      "  0.89575524 0.50572004 0.59116852 0.37606367 0.07676039 0.60361218\n",
      "  0.9725237  0.98514528 0.80024752 0.66938648 0.21512035 0.63712205\n",
      "  0.03687289 0.89403523 0.60444921 0.93322675 0.13281692 0.29181224\n",
      "  0.04532149 0.81300427 0.96482535 0.64190831 0.65690932 0.47824314\n",
      "  0.49222237 0.51504806 0.24310962 0.68313423 0.24705421 0.71235669\n",
      "  0.07682565 0.60851035 0.91555432 0.28183129 0.60470679 0.0597987\n",
      "  0.9341882  0.68222647 0.53956    0.2166694  0.17688804 0.13727236\n",
      "  0.39390284 0.3385523  0.25044636 0.58482838]\n",
      " [0.68788618 0.15288477 0.41561585 0.36091354 0.26096807 0.45809916\n",
      "  0.22529524 0.87339173 0.43300814 0.02198988 0.48418642 0.99858919\n",
      "  0.38582135 0.01954657 0.47125801 0.73187962 0.99916023 0.42927701\n",
      "  0.62569144 0.91277277 0.07430302 0.58608358 0.43625703 0.12066016\n",
      "  0.79266349 0.47556222 0.11360365 0.02980743 0.85618447 0.8378319\n",
      "  0.87273146 0.88608429 0.9912796  0.24464195 0.91600621 0.09781927\n",
      "  0.69732916 0.3002387  0.13369932 0.80569962 0.1189751  0.17226423\n",
      "  0.94955897 0.7814734  0.85420141 0.71161787 0.76056701 0.30687965\n",
      "  0.96950042 0.85758089 0.98790712 0.79863291 0.59125139 0.09804671\n",
      "  0.86936321 0.45611021 0.93577368 0.387295   0.48062902 0.64493113\n",
      "  0.93564064 0.86368544 0.01228375 0.04296794 0.00996075 0.02762109\n",
      "  0.06492474 0.06511054 0.88750814 0.65073311 0.09395928 0.36980598\n",
      "  0.26961458 0.43093094 0.41704683 0.60166745 0.86178922 0.26323738\n",
      "  0.97243516 0.27516663 0.31452968 0.94718227 0.45368198 0.73536709\n",
      "  0.69007974 0.36473011 0.15750278 0.78213168 0.37585716 0.92193658\n",
      "  0.58911084 0.77907043 0.44179228 0.65471218 0.7229921  0.1514119\n",
      "  0.02143367 0.82807464 0.89320368 0.68819705]]\n",
      "[[0.0786292 ]\n",
      " [0.70898788]\n",
      " [0.44584648]\n",
      " [0.10155053]\n",
      " [0.1707248 ]\n",
      " [0.89011991]\n",
      " [0.01799257]\n",
      " [0.00745808]\n",
      " [0.4293073 ]\n",
      " [0.07429288]\n",
      " [0.43876651]\n",
      " [0.95221879]\n",
      " [0.00744631]\n",
      " [0.06010074]\n",
      " [0.42127296]\n",
      " [0.32566076]\n",
      " [0.59076267]\n",
      " [0.68746036]\n",
      " [0.70447666]\n",
      " [0.06521442]\n",
      " [0.97272088]\n",
      " [0.82703141]\n",
      " [0.90792862]\n",
      " [0.37249949]\n",
      " [0.93532234]\n",
      " [0.98915112]\n",
      " [0.19809126]\n",
      " [0.18584408]\n",
      " [0.03477077]\n",
      " [0.88008826]\n",
      " [0.81567798]\n",
      " [0.2799385 ]\n",
      " [0.35985655]\n",
      " [0.94481577]\n",
      " [0.03527714]\n",
      " [0.21208017]\n",
      " [0.88614867]\n",
      " [0.96998653]\n",
      " [0.2501154 ]\n",
      " [0.36127678]\n",
      " [0.80061907]\n",
      " [0.94424824]\n",
      " [0.84197045]\n",
      " [0.61774909]\n",
      " [0.66048281]\n",
      " [0.46226613]\n",
      " [0.23511613]\n",
      " [0.01634123]\n",
      " [0.06478499]\n",
      " [0.51351506]\n",
      " [0.6401245 ]\n",
      " [0.46320576]\n",
      " [0.09725071]\n",
      " [0.56327219]\n",
      " [0.62273268]\n",
      " [0.59872976]\n",
      " [0.43176274]\n",
      " [0.08026077]\n",
      " [0.74722406]\n",
      " [0.1152883 ]\n",
      " [0.68811238]\n",
      " [0.33686482]\n",
      " [0.49885341]\n",
      " [0.18140098]\n",
      " [0.5379349 ]\n",
      " [0.2835398 ]\n",
      " [0.3866225 ]\n",
      " [0.721292  ]\n",
      " [0.8760823 ]\n",
      " [0.86105962]\n",
      " [0.03138385]\n",
      " [0.19394527]\n",
      " [0.9261425 ]\n",
      " [0.62153437]\n",
      " [0.98557974]\n",
      " [0.38321604]\n",
      " [0.6002921 ]\n",
      " [0.42615154]\n",
      " [0.11050223]\n",
      " [0.58110323]\n",
      " [0.65168181]\n",
      " [0.7621321 ]\n",
      " [0.03699171]\n",
      " [0.92811425]\n",
      " [0.12734131]\n",
      " [0.15220692]\n",
      " [0.78633168]\n",
      " [0.18226944]\n",
      " [0.93348503]\n",
      " [0.77107672]\n",
      " [0.75557766]\n",
      " [0.29775637]\n",
      " [0.52766319]\n",
      " [0.02662189]\n",
      " [0.49720937]\n",
      " [0.37832898]\n",
      " [0.25693346]\n",
      " [0.41915843]\n",
      " [0.02621714]\n",
      " [0.52748419]]\n",
      "[1.]\n",
      "epoch =  1\n",
      "[1.]\n",
      "epoch =  2\n",
      "[1.]\n",
      "epoch =  3\n",
      "[1.]\n",
      "epoch =  4\n",
      "[1.]\n",
      "epoch =  5\n",
      "[1.]\n",
      "epoch =  6\n",
      "[1.]\n",
      "epoch =  7\n",
      "[1.]\n",
      "epoch =  8\n",
      "[1.]\n",
      "epoch =  9\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "a.train(epochs = 10, lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.layers[0].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.layers[1].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train.reshape(-1, 28*28) / 255.0, x_test.reshape(-1, 28*28) / 255.0\n",
    "y_train = np.transpose(np.array([np.where(y_train == element, 1, 0) for element in np.unique(y_train)]))\n",
    "y_test = np.transpose(np.array([np.where(y_test == element, 1, 0) for element in np.unique(y_test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[1].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "model = Neural_Net()\n",
    "model.add_layer(10, activation = 'sigmoid')\n",
    "model.set_input(x_train)\n",
    "model.set_output(y_train)\n",
    "t0 = time.time()\n",
    "model.train(lr = 1e-5, epochs = 2, batch_size = None)\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minpy.numpy as np\n",
    "from minpy.context import cpu, gpu\n",
    "with gpu(0):\n",
    "    model = Neural_Net()\n",
    "    model.add_layer(10, activation = 'sigmoid')\n",
    "    model.set_input(np.array(x_train))\n",
    "    model.set_output(np.array(y_train))\n",
    "    t2 = time.time()\n",
    "    model.train(lr = 1e-5, epochs = 2, batch_size = None)\n",
    "    t3 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"run on cpu: %.6f s/iter\" % ((t1 - t0) / n))\n",
    "print(\"run on gpu: %.6f s/iter\" % ((t3 - t2) / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(lr = 1e-5, epochs = 2, batch_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_input(x_train)\n",
    "model.set_output(y_train)\n",
    "model.feed_forward()\n",
    "predict = np.argmax(model.layers[-1].output, axis=1)\n",
    "y = np.argmax(y_train, axis=1)\n",
    "accuracy = (y == predict)\n",
    "score = sum(np.where(accuracy == True, 1, 0))/len(y)\n",
    "print('Test Accuracy =', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_input(x_test)\n",
    "model.set_output(y_test)\n",
    "model.feed_forward()\n",
    "predict = np.argmax(model.layers[-1].output, axis=1)\n",
    "y = np.argmax(y_test, axis=1)\n",
    "accuracy = (y == predict)\n",
    "score = sum(np.where(accuracy == True, 1, 0))/len(y)\n",
    "print('Test Accuracy =', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
